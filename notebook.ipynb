{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "from config import logger\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def fetch_audio_files(path):\n",
    "    audio_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in sorted(files):\n",
    "            if f.endswith('.wav'):\n",
    "                audio_files.append(os.path.join(root, f))\n",
    "    logger.info(f\"Successfully fetched {len(audio_files)} (.wav) audio files!\")\n",
    "    return audio_files\n",
    "\n",
    "\n",
    "def get_user_input(prompt, choices):\n",
    "    while True:\n",
    "        user_input = input(prompt)\n",
    "        if user_input.lower() in choices:\n",
    "            return user_input.lower()\n",
    "        else:\n",
    "            logger.info('Invalid input. Please try again.')\n",
    "\n",
    "\n",
    "def df_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, index=False)\n",
    "    logger.info(f\"Writing {file_path}...\")\n",
    "\n",
    "\n",
    "def add_train_scores(df):\n",
    "    text_data = df\n",
    "    logger.debug(text_data)\n",
    "    scores_df = pd.read_csv(config.diagnosis_train_scores)\n",
    "    scores_df = scores_df.rename(columns={'adressfname': 'addressfname', 'dx': 'diagnosis'})\n",
    "    scores_df = binarize_labels(scores_df)\n",
    "    logger.debug(scores_df)\n",
    "    output = pd.merge(text_data,\n",
    "                      scores_df[['addressfname', 'mmse', 'diagnosis']],  # We don't want the key column here\n",
    "                      on='addressfname',\n",
    "                      how='inner')\n",
    "\n",
    "    logger.debug(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def binarize_labels(df):\n",
    "    df['diagnosis'] = [1 if label == 'ad' else 0 for label in df['diagnosis']]\n",
    "    df_majority = df[df['diagnosis'] == 1]  \n",
    "    df_minority = df[df['diagnosis'] == 0]  \n",
    "    df_majority_downsampled = resample(df_majority,\n",
    "                                       replace=False,  \n",
    "                                       n_samples=len(df_minority), \n",
    "                                       random_state=42)  \n",
    "\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    plt.show()\n",
    "    return df_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class Formatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        if record.levelno == logging.INFO:\n",
    "            self._style._fmt = \"%(message)s\"\n",
    "        else:\n",
    "            color = {\n",
    "                logging.WARNING: 33,\n",
    "                logging.ERROR: 31,\n",
    "                logging.FATAL: 31,\n",
    "                logging.DEBUG: 36\n",
    "            }.get(record.levelno, 0)\n",
    "            self._style._fmt = f\"\\033[{color}m%(levelname)s [%(filename)s:%(lineno)d]\\033[0m: %(message)s\"\n",
    "        return super().format(record) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from config import logger\n",
    "from utils import fetch_audio_files, df_to_csv, add_train_scores\n",
    "from pathlib import Path\n",
    "import whisper\n",
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def transcribe():\n",
    "    whisper_model = whisper.load_model(config.whisper_model_name)\n",
    "\n",
    "    logger.info(\"Initiating transcription...\")\n",
    "\n",
    "    diagnosis_train_audio_files = fetch_audio_files(config.diagnosis_train_data)\n",
    "    logger.debug(diagnosis_train_audio_files)\n",
    "    diagnosis_test_audio_files = fetch_audio_files(config.diagnosis_test_data)\n",
    "    logger.debug(diagnosis_test_audio_files)\n",
    "\n",
    "    write_transcription(diagnosis_train_audio_files, config.diagnosis_train_transcription_dir, whisper_model)\n",
    "    write_transcription(diagnosis_test_audio_files, config.diagnosis_test_transcription_dir, whisper_model)\n",
    "\n",
    "    train_df = transcription_to_df(config.diagnosis_train_transcription_dir)\n",
    "    train_df = add_train_scores(train_df)\n",
    "\n",
    "    test_df = transcription_to_df(config.diagnosis_test_transcription_dir)\n",
    "\n",
    "    df_to_csv(train_df, config.train_scraped_path)\n",
    "    df_to_csv(test_df, config.test_scraped_path)\n",
    "\n",
    "    logger.info(\"Transcription done.\")\n",
    "\n",
    "\n",
    "def write_transcription(audio_files, transcription_dir, whisper_model):\n",
    "    for audio_file in audio_files:\n",
    "        filename = Path(audio_file).stem\n",
    "        transcription_file = (transcription_dir / filename).resolve()\n",
    "\n",
    "        if not transcription_file.exists():\n",
    "            result = whisper_model.transcribe(audio_file, fp16=False)\n",
    "            transcription_str = str(result[\"text\"])\n",
    "\n",
    "            transcription_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            transcription_file.write_text(transcription_str)\n",
    "            logger.info(f\"Transcribed {transcription_file}...\")\n",
    "\n",
    "\n",
    "def transcription_to_df(data_dir):\n",
    "    texts = []\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            with codecs.open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "                texts.append((file, text))\n",
    "\n",
    "    df = pd.DataFrame(texts, columns=['addressfname', 'transcript'])\n",
    "    df['transcript'] = df['transcript'].str.replace('\\n', ' ').replace('\\\\n', ' ').replace('  ', ' ')\n",
    "    df = df.sort_values(by='addressfname')\n",
    "    df = df.reset_index(drop=True)\n",
    "    logger.debug(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import typing\n",
    "from pathlib import Path\n",
    "from formatter import Formatter\n",
    "import tiktoken\n",
    "\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(Formatter())\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "dirname = Path(__file__).parent.resolve()\n",
    "\n",
    "whisper_model_name = None\n",
    "whisper_model = None\n",
    "\n",
    "max_tokens = 500\n",
    "embedding_engine = 'text-embedding-ada-002'\n",
    "\n",
    "n_splits = 10\n",
    "\n",
    "data_dir = (dirname / \"ADReSSo\").resolve()\n",
    "\n",
    "diagnosis_train_data = (\n",
    "        data_dir / \"diagnosis-train\" / \"diagnosis\" / \"train\" / \"audio\").resolve()  # Dementia and control group\n",
    "diagnosis_test_data = (\n",
    "        data_dir / \"diagnosis-test\" / \"diagnosis\" / \"test-dist\" / \"audio\").resolve()  # Test data\n",
    "diagnosis_train_scores = (\n",
    "        data_dir / \"diagnosis-train\" / \"diagnosis\" / \"train\" / \"adresso-train-mmse-scores.csv\").resolve()\n",
    "empty_test_results_file = (data_dir / \"diagnosis-test\" / \"diagnosis\" / \"test-dist\" / \"test_results_task1.csv\").resolve()\n",
    "test_results_task1 = (data_dir / \"task1.csv\").resolve()\n",
    "\n",
    "\n",
    "decline_data = (\n",
    "        data_dir / \"progression-train\" / \"progression\" / \"train\" / \"audio\" / \"decline\").resolve()  \n",
    "no_decline_data = (\n",
    "        data_dir / \"progression-train\" / \"progression\" / \"train\" / \"audio\" / \"no_decline\").resolve()  \n",
    "\n",
    "\n",
    "transcription_dir = (dirname / \"processed\" / \"transcription\").resolve()\n",
    "diagnosis_train_transcription_dir = (transcription_dir / \"train\").resolve()\n",
    "diagnosis_test_transcription_dir = (transcription_dir / \"test\").resolve()\n",
    "train_scraped_path = (dirname / \"processed\" / \"train_scraped.csv\").resolve()\n",
    "test_scraped_path = (dirname / \"processed\" / \"test_scraped.csv\").resolve()\n",
    "train_embeddings_path = (dirname / \"processed\" / \"train_embeddings.csv\").resolve()\n",
    "test_embeddings_path = (dirname / \"processed\" / \"test_embeddings.csv\").resolve()\n",
    "\n",
    "embedding_results_dir = (dirname / \"results\" / \"embedding\").resolve()\n",
    "models_size_file = (embedding_results_dir / 'embedding_models_size.csv').resolve()\n",
    "\n",
    "\n",
    "def set_up():\n",
    "    logger.info(\"Loading cl100k_base tokenizer...\")\n",
    "    logger.info(f\"Max tokens per embedding: {max_tokens}.\")\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    logger.info(f\"Loading GPT embedding engine {embedding_engine}...\")\n",
    "\n",
    "    Path(dirname / \"processed\").resolve().mkdir(exist_ok=True)\n",
    "    Path(dirname / \"results\").resolve().mkdir(exist_ok=True)\n",
    "    embedding_results_dir.mkdir(exist_ok=True)\n",
    "    Path(embedding_results_dir / 'plots').resolve().mkdir(exist_ok=True)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def secret_key() -> typing.Optional[str]:\n",
    "    value = os.environ.get('OPENAI_API_KEY', None)\n",
    "\n",
    "    if not value:\n",
    "        logger.warning(\"Optional environment variable 'OPENAI_API_KEY' is missing.\")\n",
    "\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import config\n",
    "from config import logger\n",
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "\n",
    "def tokenization(df, tokenizer):\n",
    "    df_columns = df.columns\n",
    "    df['n_tokens'] = df['transcript'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    df['n_tokens'].hist()\n",
    "    plt.show()\n",
    "\n",
    "    def split_into_many(text, max_tokens):\n",
    "        sentences = text.split('. ')\n",
    "\n",
    "        chunks = []\n",
    "        tokens_so_far = 0\n",
    "        chunk = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            token = len(tokenizer.encode(\" \" + sentence))\n",
    "\n",
    "            if tokens_so_far + token > max_tokens:\n",
    "                chunks.append(\". \".join(chunk) + \".\")\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "\n",
    "            if token > max_tokens:\n",
    "                continue\n",
    "\n",
    "            chunk.append(sentence)\n",
    "            tokens_so_far += token + 1\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    shortened = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row['transcript'] is None:\n",
    "            continue\n",
    "\n",
    "        if row['n_tokens'] > config.max_tokens:\n",
    "            row_chunks = split_into_many(row['transcript'], max_tokens=config.max_tokens)\n",
    "            for chunk in row_chunks:\n",
    "                columns_to_append = {col: row[col] for col in df_columns if col in row}\n",
    "                columns_to_append['transcript'] = chunk\n",
    "                shortened.append(columns_to_append)\n",
    "        else:\n",
    "            shortened.append({col: row[col] for col in df_columns})\n",
    "\n",
    "    df_shortened = pd.DataFrame(shortened)\n",
    "    df_shortened['n_tokens'] = df_shortened['transcript'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    df_shortened['n_tokens'].hist()\n",
    "    plt.show()\n",
    "\n",
    "    logger.debug(df_shortened)\n",
    "    return df_shortened\n",
    "\n",
    "\n",
    "def create_embeddings(df):\n",
    "    df['embedding'] = df['transcript'].apply(\n",
    "        lambda x: openai.Embedding.create(input=x, engine=config.embedding_engine)['data'][0]['embedding'])\n",
    "    df = df.drop('transcript', axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def embeddings_exists():\n",
    "    if config.train_embeddings_path.is_file() and config.test_embeddings_path.is_file():\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139df470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, make_scorer, recall_score, precision_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    KFold, train_test_split, GridSearchCV, cross_validate\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import config\n",
    "from config import logger\n",
    "\n",
    "def embeddings_to_array(embeddings_file):\n",
    "    df = pd.read_csv(embeddings_file)\n",
    "    df['embedding'] = df['embedding'].apply(eval).apply(np.array)\n",
    "    logger.debug(df.head())\n",
    "    return df\n",
    "\n",
    "\n",
    "def cross_validation(model, _X, _y, _cv):\n",
    "    _scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),  \n",
    "        'precision': make_scorer(precision_score, average='weighted'), \n",
    "        'recall': make_scorer(recall_score, average='weighted'),        \n",
    "        'f1_score': make_scorer(f1_score, average='macro')  \n",
    "    }\n",
    "\n",
    "    scores = cross_validate(estimator=model,\n",
    "                            X=_X,\n",
    "                            y=_y,\n",
    "                            cv=_cv,\n",
    "                            scoring=_scoring,\n",
    "                            return_train_score=True)\n",
    "\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        train_scores = scores[f'test_{metric}']\n",
    "        train_scores_mean = round(train_scores.mean(), 3)\n",
    "        train_scores_std = round(train_scores.std(), 3)\n",
    "\n",
    "        test_scores = scores[f'test_{metric}']\n",
    "        test_scores_mean = round(test_scores.mean(), 3)\n",
    "\n",
    "        result[f'train_{metric}'] = train_scores\n",
    "        result[f'train_{metric}_mean'] = train_scores_mean\n",
    "        result[f'train_{metric}_std'] = train_scores_std\n",
    "\n",
    "        result[f'test_{metric}'] = test_scores\n",
    "        result[f'test_{metric}_mean'] = test_scores_mean\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def classify_embedding(train_data, test_data, _n_splits):\n",
    "    logger.info(\"Initiating classification with GPT-3 text embeddings...\")\n",
    "    y_train = train_data['diagnosis'].values\n",
    "    X_train = train_data['embedding'].to_list()\n",
    "    X_test = test_data['embedding'].to_list()\n",
    "\n",
    "    baseline_score = dummy_stratified_clf(X_train, y_train)\n",
    "    logger.debug(f\"Baseline performance of the dummy classifier: {baseline_score}\")\n",
    "\n",
    "    models = [SVC(), LogisticRegression(), RandomForestClassifier()]\n",
    "    names = ['SVC', 'LR', 'RF']\n",
    "\n",
    "    cv = KFold(n_splits=_n_splits, random_state=42, shuffle=True)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=['Set', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "    models_size_df = pd.DataFrame(columns=['Model', 'Size'])\n",
    "\n",
    "    logger.info(\"Beginning to train models using GPT embeddings...\")\n",
    "\n",
    "    total_models_size = 0\n",
    "\n",
    "    for model, name in zip(models, names):\n",
    "        logger.info(f\"Initiating {name}...\")\n",
    "\n",
    "        best_params = hyperparameter_optimization(X_train, y_train, cv, model, name)\n",
    "        model.set_params(**best_params)\n",
    "        scores = cross_validation(model, X_train, y_train, cv)\n",
    "        results_df = results_to_df(name, scores, results_df)\n",
    "\n",
    "        visualize_results(_n_splits, name, scores, (config.embedding_results_dir / \"plots\").resolve())\n",
    "\n",
    "        model_size = len(pickle.dumps(model, -1))\n",
    "        logger.debug(f\"Model size of {name} before training: {model_size} bytes.\")\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        model_size = len(pickle.dumps(model, -1))\n",
    "        logger.debug(f\"Model size of {name} after training: {model_size} bytes.\")\n",
    "        total_models_size += model_size\n",
    "\n",
    "        models_size_df = pd.concat([models_size_df, pd.DataFrame([{'Model': name,\n",
    "                                                                   'Size': f\"{model_size} B\",\n",
    "                                                                   }])], ignore_index=True)\n",
    "\n",
    "        model_test_results = pd.read_csv(config.empty_test_results_file)\n",
    "\n",
    "        model_predictions = model.predict(X_test)\n",
    "\n",
    "        filename_to_prediction = {}\n",
    "\n",
    "        for filename, prediction in zip(test_data['addressfname'], model_predictions):\n",
    "            filename_to_prediction[filename] = 'ProbableAD' if prediction == 1 else 'Control'\n",
    "        model_test_results['Prediction'] = model_test_results['ID'].map(filename_to_prediction)\n",
    "        model_test_results_csv = (config.embedding_results_dir / f'task1_{name}.csv').resolve()\n",
    "        model_test_results.to_csv(model_test_results_csv, index=False)\n",
    "        logger.info(f\"Writing {model_test_results_csv}...\")\n",
    "        evaluate_similarity(name, model_test_results)\n",
    "\n",
    "    logger.info(\"Training using GPT embeddings done.\")\n",
    "\n",
    "    results_df = results_df.sort_values(by='Set', ascending=False)\n",
    "    results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{'Set': 'Test',\n",
    "                                                       'Model': 'Dummy',\n",
    "                                                       'Accuracy': baseline_score,\n",
    "                                                       }])], ignore_index=True)\n",
    "\n",
    "    embedding_results_file = (config.embedding_results_dir / 'embedding_results.csv').resolve()\n",
    "    results_df.to_csv(embedding_results_file)\n",
    "    logger.info(f\"Writing {embedding_results_file}...\")\n",
    "\n",
    "    logger.debug(f\"Total size of all models: {total_models_size}.\")\n",
    "    models_size_df = pd.concat([models_size_df, pd.DataFrame([{'Model': 'Total',\n",
    "                                                               'Size': f'{total_models_size} B',\n",
    "                                                               }])], ignore_index=True)\n",
    "\n",
    "    models_size_df.to_csv(config.models_size_file)\n",
    "    logger.info(f\"Writing {config.models_size_file}...\")\n",
    "\n",
    "    logger.info(\"Classification with GPT-3 text embeddings done.\")\n",
    "\n",
    "\n",
    "def evaluate_similarity(name, model_test_results):\n",
    "    test_results_task1 = pd.read_csv(config.test_results_task1)\n",
    "    real_diagnoses = test_results_task1['Dx']\n",
    "    predicted_diagnoses = model_test_results['Prediction']\n",
    "    matching_values = (real_diagnoses == predicted_diagnoses).sum()\n",
    "    total_values = len(real_diagnoses)\n",
    "    similarity_percentage = (matching_values / total_values) * 100\n",
    "    logger.info(f\"The similarity between the real and predicted diagnoses using model {name} \"\n",
    "                f\"is {similarity_percentage:.2f}%.\")\n",
    "\n",
    "\n",
    "def hyperparameter_optimization(X_train, y_train, cv, model, name):\n",
    "    lr_param_grid, rf_param_grid, svc_param_grid = param_grids()\n",
    "    grid_search = None\n",
    "    if name == 'SVC':\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=svc_param_grid, cv=cv, n_jobs=-1, error_score=0.0)\n",
    "    elif name == 'LR':\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=lr_param_grid, cv=cv, n_jobs=-1, error_score=0.0)\n",
    "    elif name == 'RF':\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=rf_param_grid, cv=cv, n_jobs=-1, error_score=0.0)\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def param_grids():\n",
    "    svc_param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    }\n",
    "    lr_param_grid = [\n",
    "        {'penalty': ['l1', 'l2'],\n",
    "         'C': np.logspace(-4, 4, 20),\n",
    "         'solver': ['liblinear'],\n",
    "         'max_iter': [100, 200, 500, 1000]},\n",
    "        {'penalty': ['l2'],\n",
    "         'C': np.logspace(-4, 4, 20),\n",
    "         'solver': ['lbfgs'],\n",
    "         'max_iter': [200, 500, 1000]},\n",
    "    ]\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [25, 50, 100, 150],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'max_leaf_nodes': [3, 6, 9],\n",
    "    }\n",
    "    return lr_param_grid, rf_param_grid, svc_param_grid\n",
    "\n",
    "\n",
    "def visualize_results(_n_splits, name, results, save_dir):\n",
    "    plot_accuracy_path = (save_dir / f'plot_accuracy_{name}.png').resolve()\n",
    "    plot_precision_path = (save_dir / f'plot_precision_{name}.png').resolve()\n",
    "    plot_recall_path = (save_dir / f'plot_recall_{name}.png').resolve()\n",
    "    plot_f1_path = (save_dir / f'plot_f1_{name}.png').resolve()\n",
    "    plot_result(name,\n",
    "                \"Accuracy\",\n",
    "                f\"Accuracy scores in {_n_splits} Folds\",\n",
    "                results[\"train_accuracy\"],\n",
    "                results[\"test_accuracy\"],\n",
    "                plot_accuracy_path)\n",
    "    plot_result(name,\n",
    "                \"Precision\",\n",
    "                f\"Precision scores in {_n_splits} Folds\",\n",
    "                results[\"train_precision\"],\n",
    "                results[\"test_precision\"],\n",
    "                plot_precision_path)\n",
    "    plot_result(name,\n",
    "                \"Recall\",\n",
    "                f\"Recall scores in {_n_splits} Folds\",\n",
    "                results[\"train_recall\"],\n",
    "                results[\"test_recall\"],\n",
    "                plot_recall_path)\n",
    "    plot_result(name,\n",
    "                \"F1\",\n",
    "                f\"F1 Scores in {_n_splits} Folds\",\n",
    "                results[\"train_f1_score\"],\n",
    "                results[\"test_f1_score\"],\n",
    "                plot_f1_path)\n",
    "\n",
    "\n",
    "def results_to_df(name, scores, results_df):\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{'Set': 'Train',\n",
    "                                                       'Model': name,\n",
    "                                                       'Accuracy': f\"{scores['train_accuracy_mean']} \"\n",
    "                                                                   f\"({scores['train_accuracy_std']})\",\n",
    "                                                       'Precision': f\"{scores['train_precision_mean']} \"\n",
    "                                                                    f\"({scores['train_precision_std']})\",\n",
    "                                                       'Recall': f\"{scores['train_recall_mean']} \"\n",
    "                                                                 f\"({scores['train_recall_std']})\",\n",
    "                                                       'F1': f\"{scores['train_f1_score_mean']} \"\n",
    "                                                             f\"({scores['train_f1_score_std']})\",\n",
    "                                                       }])], ignore_index=True)\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{'Set': 'Test',\n",
    "                                                       'Model': name,\n",
    "                                                       'Accuracy': scores['test_accuracy_mean'],\n",
    "                                                       'Precision': scores['test_precision_mean'],\n",
    "                                                       'Recall': scores['test_recall_mean'],\n",
    "                                                       'F1': scores['test_f1_score_mean']\n",
    "                                                       }])], ignore_index=True)\n",
    "    return results_df\n",
    "\n",
    "def plot_result(x_label, y_label, plot_title, train_data, val_data, savefig_path=None):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    labels = [\"1st Fold\", \"2nd Fold\", \"3rd Fold\", \"4th Fold\", \"5th Fold\", \"6th Fold\", \"7th Fold\", \"8th Fold\",\n",
    "              \"9th Fold\", \"10th Fold\"]\n",
    "    X_axis = np.arange(len(labels))\n",
    "    plt.ylim(0.40000, 1)\n",
    "    plt.bar(X_axis - 0.2, train_data, 0.4, color='blue', label='Training')\n",
    "    plt.bar(X_axis + 0.2, val_data, 0.4, color='red', label='Validation')\n",
    "    plt.title(plot_title, fontsize=30)\n",
    "    plt.xticks(X_axis, labels)\n",
    "    plt.xlabel(x_label, fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    if savefig_path is not None:\n",
    "        fig.savefig(savefig_path, dpi=fig.dpi)\n",
    "\n",
    "\n",
    "def dummy_stratified_clf(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    stratified_clf = DummyClassifier(strategy='stratified').fit(X_train, y_train)\n",
    "\n",
    "    score = round(stratified_clf.score(X_test, y_test), 3)\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import classification\n",
    "import config\n",
    "import embedding\n",
    "import transcribe\n",
    "from config import logger\n",
    "from utils import get_user_input, df_to_csv\n",
    "\n",
    "\n",
    "def main():\n",
    "    openai.api_key = config.secret_key()\n",
    "\n",
    "    tokenizer = config.set_up()\n",
    "\n",
    "    yes_choices = [\"yes\", \"y\"]\n",
    "    no_choices = [\"no\", \"n\"]\n",
    "\n",
    "    transcription_prompt = get_user_input(\"Would you like to transcribe the audio files? (yes/no): \",\n",
    "                                          yes_choices\n",
    "                                          + no_choices)\n",
    "\n",
    "    if transcription_prompt in yes_choices:\n",
    "        logger.info(\"If there is already a transcription, please delete it first. \"\n",
    "                    \"Otherwise, already transcribed files will be skipped, no matter which model was used for it.\")\n",
    "        whisper_model_choices = [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
    "        whisper_model_prompt = get_user_input(\"Which Whisper model should be used for transcription? \"\n",
    "                                              \"(tiny/base/small/medium/large): \", whisper_model_choices)\n",
    "        config.whisper_model_name = whisper_model_prompt\n",
    "        transcribe.transcribe()\n",
    "    else:\n",
    "        logger.info(\"Transcription skipped.\")\n",
    "\n",
    "    classification_prompt = get_user_input(\"Would you like the classification to be (re-)run? (yes/no): \", yes_choices\n",
    "                                           + no_choices)\n",
    "\n",
    "    if classification_prompt in yes_choices:\n",
    "        create_embeddings = False\n",
    "        if embedding.embeddings_exists():\n",
    "            embedding_prompt = get_user_input(\"There already seem to exist some embeddings. \"\n",
    "                                              \"Would you like to create new embeddings? (yes/no): \",\n",
    "                                              yes_choices + no_choices)\n",
    "            if embedding_prompt in yes_choices:\n",
    "                create_embeddings = True\n",
    "            else:\n",
    "                logger.info(\"Embedding skipped.\")\n",
    "        else:\n",
    "            create_embeddings = True\n",
    "            logger.info(\"Embeddings not found. Creating embeddings automatically...\")\n",
    "\n",
    "        if create_embeddings:\n",
    "            logger.info(\"Initiating embedding...\")\n",
    "            train_df = pd.read_csv(config.train_scraped_path)\n",
    "            test_df = pd.read_csv(config.test_scraped_path)\n",
    "\n",
    "            train_tokenization = embedding.tokenization(train_df, tokenizer)\n",
    "            test_tokenization = embedding.tokenization(test_df, tokenizer)\n",
    "\n",
    "            train_embeddings = embedding.create_embeddings(train_tokenization)\n",
    "            test_embeddings = embedding.create_embeddings(test_tokenization)\n",
    "\n",
    "            df_to_csv(train_embeddings, config.train_embeddings_path)  # Specify file paths\n",
    "            df_to_csv(test_embeddings, config.test_embeddings_path)\n",
    "\n",
    "            logger.info(\"Embedding done.\")\n",
    "\n",
    "        train_embeddings_array = classification.embeddings_to_array(config.train_embeddings_path)\n",
    "        test_embeddings_array = classification.embeddings_to_array(config.test_embeddings_path)\n",
    "\n",
    "        classification.classify_embedding(train_embeddings_array, test_embeddings_array, config.n_splits)\n",
    "    else:\n",
    "        logger.info(\"Classification skipped.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
